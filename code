import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn; seaborn.set() #基于matlab上实现可封装的统计图表
%matplotlib inline

#读取数据
train = pd.read_csv("D:/BaiduNetdiskDownload/archive/bitcoin_price_Training - Training.csv")
test = pd.read_csv("D:/BaiduNetdiskDownload/archive/bitcoin_price_1week_Test - Test.csv")

#对数据进行打印
train
test
print(train.shape)
print(test.shape)
train.head()
train.tail()
#将数据进行反转后对数据再次进行观察
train = train[::-1] 
test = test[::-1]
train.head()

from dateutil.parser import parse#日期解析器 功能为将日期转化成字符串
from datetime import datetime


#对数据进行处理，将其中的日期部分利用调用的库将其转化成字符串。
def convert(date):
    holder = []
    for i in date:
        tp = parse(i).timestamp()
        dt = datetime.fromtimestamp(tp)
        holder.append(dt)
    return np.array(holder)
    
    
    #获取数据的属性值并且进行处理
date = train['Date'].values
date_n = convert(date)
#完整性检测
print(len(date_n) == train.shape[0])

train['Date'] = date_n
train.head()
train = train.set_index('Date')
train.head()
train.describe()
# 检测是否存在空值，即缺失值。
train.isnull().any()

#绘制线性图像和log函数图像
plt.figure(num=None, figsize=(20, 6))
plt.subplot(1,2,1)
ax = train['Close'].plot(style=['-'])
ax.lines[0].set_alpha(0.3)
ax.set_ylim(0, np.max(train['Close'] + 100))
plt.xticks(rotation=90)
plt.title("No scaling")
ax.legend()
plt.subplot(1,2,2)
ax = train['Close'].plot(style=['-'])
ax.lines[0].set_alpha(0.3)
ax.set_yscale('log')
ax.set_ylim(0, np.max(train['Close'] + 100))
plt.xticks(rotation=90)
plt.title("logarithmic scale")
ax.legend()

#绘制平均价格和年底的价格

close = train['Close']
close.plot(alpha=0.5, style='-')
close.resample('BA').mean().plot(style=':')
close.asfreq('BA').plot(style='--')#asfreq()方法来转换时期的频率，一年中的占比
plt.yscale('log')
plt.title("logarithmic scale")
plt.legend(['close-price', 'resample', 'asfreq'], 
           loc='upper left')
# 'resample'-- average of the previous year
# 'asfreq' -- value at the end of the year

#绘制投资回报率
ROI = 100 * (close.tshift(-365) / close - 1)#tshift()函数用于移动时间索引
ROI.plot()
plt.ylabel('% Return on Investment');

#绘制移动平均线，ema指数移动平均线，sma简单移动平均线
rolling = close.rolling(200, center=True) #生成滑动窗口，对时间窗口进行滑动获取

data = pd.DataFrame({'input': close, 
                     '200days rolling_mean': rolling.mean(), 
                     '200days rolling_std': rolling.std()})

ax = data.plot(style=['-', '--', ':'])
ax.set_yscale('log')
ax.set_title("SMA on log scale")
rolling = close.rolling(365, center=True)
ax.lines[0].set_alpha(0.3)

ax = data.plot(style=['-', '--', ':'])
ax.set_title("SMA on raw data")
ax.lines[0].set_alpha(0.3)

pd.DataFrame.ewm(close, span = 200).mean()

data = pd.DataFrame({'input': close, 
                     '200days rolling_mean': rolling.mean(), 
                     '200days rolling_std': rolling.std()})

ax = data.plot(style=['-', '--', ':'])
ax.set_yscale('log')
ax.set_title("EMA on log scale")
ax.lines[0].set_alpha(0.3)

ax = data.plot(style=['-', '--', ':'])
ax.set_title("EMA on raw data")
ax.lines[0].set_alpha(0.3)


#绘制滞后图
from pandas.plotting import lag_plot
lag_plot(close)

#绘制强相关性图
from pandas.plotting import autocorrelation_plot
autocorrelation_plot(close)

from pandas import Series
from statsmodels.graphics.tsaplots import plot_pacf

plot_pacf(close, lags=50)

#利用AIC方法对数据进行评估，得到其最合适的阶数

from statsmodels.tsa.ar_model import AR
from sklearn.metrics import mean_squared_error

test = test['Close'].values
train_pr = train['Close'].values
# train and fit autoregression
model = AR(train_pr)
model_fit = model.fit()

print("Lag: %s" % model_fit.k_ar)
print("Coefficients: %s" % model_fit.params)

pred = model_fit.predict(start=len(train), end=len(train_pr)+len(test)-1, dynamic=False)

mse = mean_squared_error(test, pred)
print("Test MSE {0:.3f}".format(mse))
pred = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)
pred
plt.plot(test, label='true value')
plt.plot(pred, color='red', label='prediction')
plt.title("Autoregressive model")
plt.legend()

RNN

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns; sns.set()
# Input data files are available in the "../input/" directory.

train = pd.read_csv("D:/BaiduNetdiskDownload/archive/bitcoin_price_Training - Training.csv")
test = pd.read_csv("D:/BaiduNetdiskDownload/archive/bitcoin_price_1week_Test - Test.csv")

train = train[::-1]
test = test[::-1]

train = train['Close'].values.astype('float32')
test = test['Close'].values.astype('float32')
from sklearn.preprocessing import StandardScaler

# reshape data to scale the point
train = train.reshape(-1, 1)
test = test.reshape(-1, 1)

scaler = StandardScaler()
train_n = scaler.fit_transform(train)
test_n = scaler.transform(test)
print(train_n.shape)
print(test_n.shape)
def generator(data, lookback, delay, min_index, max_index, 
              shuffle=False, batch_size=128, step=1):
    if max_index is None:
        max_index = len(data) - delay - 1
    i = min_index + lookback
    while 1:
        if shuffle:
            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)
        else:
            if i + batch_size >= max_index:
                i = min_index + lookback
                
            rows = np.arange(i, min(i + batch_size, max_index))
            i += len(rows)
        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))
        targets = np.zeros((len(rows),))
        for j, row in enumerate(rows):
            indices = range(rows[j] - lookback, rows[j], step)
            samples[j] = data[indices]
            targets[j] = data[rows[j] + delay]
        yield samples, targets
        
lookback = 24
step = 1
delay = 7
batch_size = 128
train_gen = generator(train_n, lookback=lookback, delay=delay,
    min_index=0, max_index=1000, shuffle=True, step=step,
batch_size=batch_size)
val_gen = generator(train_n, lookback=lookback, delay=delay,
    min_index=1001, max_index=None, step=step, batch_size=batch_size)
test_gen = generator(test_n, lookback=lookback, delay=delay,
    min_index=0, max_index=None, step=step, batch_size=batch_size)
# This is how many steps to draw from `val_gen` in order to see the whole validation set:
val_steps = (len(train_n) - 1001 - lookback) // batch_size
# This is how many steps to draw from `test_gen` in order to see the whole test set:
test_steps = (len(test_n) - lookback) // batch_size

# reproducibility (make sure each time training is occurred, the result will be the same)
np.random.seed(7)

from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop

model = Sequential()
model.add(layers.GRU(32,
                     dropout=0.2,
                     recurrent_dropout=0.2,
                     input_shape=(None, train_n.shape[-1])))
model.add(layers.Dense(1))
model.compile(optimizer=RMSprop(), loss='mae')
model.summary()

history = model.fit_generator(train_gen,
                              steps_per_epoch=500,
                              epochs=40,
                              validation_data=val_gen,
                              validation_steps=val_steps)
                              
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(loss))
plt.figure()
plt.plot(epochs, loss, 'blue', label='train loss')
plt.plot(epochs, val_loss, 'orange', label='test loss')
plt.title('Training and validation loss')
plt.legend()

train_re = train_n.reshape(-1,1,1)
pred = model.predict(train_re)
pred = model.predict(train_re)
pred = scaler.inverse_transform(pred)
plt.plot(range(len(train_re)), train, label='train')
plt.plot(range(len(train_re)), pred, label='prediction')
plt.legend()
plt.title("Prediction on training data")
